# 豆包实时流式语音识别集成方案

## 1. 后端实现

### 1.1 创建豆包实时流式语音识别集成类
- 基于现有的`sauc_websocket_demo.py`创建`DoubaoStreamingVoiceIntegration`类
- 实现实时流式语音识别功能，使用WebSocket与前端通信
- 集成到现有的语音服务框架中，作为新的服务提供商选项
- **核心实现**：
  - 重写`transcribe`方法，支持流式处理
  - 实现WebSocket服务端，处理前端连接
  - 集成豆包WebSocket API，实现实时语音识别
  - 处理音频块的边界情况，确保格式完整性

### 1.2 添加流式语音识别WebSocket API端点
- 在`voice_api.py`中添加新的WebSocket端点用于流式语音识别
- **核心实现**：
  - 添加`/voice/stream` WebSocket端点
  - 支持实时接收前端发送的音频数据
  - 实时返回识别结果给前端
  - 处理连接建立、数据传输、错误处理等场景
  - 实现会话管理，支持多个客户端同时连接

### 1.3 修改配置
- 在`config.py`中添加豆包流式语音识别的配置选项
- **核心实现**：
  - 将默认语音服务提供商改为`doubao_streaming`
  - 保留whisper和普通豆包语音识别作为备选选项
  - 更新`AVAILABLE_PROVIDERS`列表，添加豆包流式语音识别
  - 添加豆包WebSocket API的URL配置

### 1.4 音频处理优化
- 实现前端发送的原始音频数据处理
- **核心实现**：
  - 支持多种音频格式，自动转换为豆包API要求的格式
  - 实现音频数据的实时处理和压缩
  - 确保音频数据的实时性和准确性
  - **解决音频块边界问题**：
    - 接收前端发送的完整WAV格式音频块
    - 验证每个音频块的格式完整性
    - 处理格式错误的音频块，提供错误提示

## 2. 前端实现

### 2.1 创建语音输入组件
- 创建`VoiceButton.tsx`组件，集成到现有ChatPanel的输入区域
- **核心实现**：
  - 使用Web Audio API实现前端录音功能，无需依赖pyaudio
  - 实现WebSocket客户端，与后端流式语音识别服务通信
  - 显示录音状态、语音波形等视觉反馈
  - 集成到现有UI中，保持风格一致

### 2.2 修改ChatPanel组件
- 在现有输入区域添加语音输入按钮
- **核心实现**：
  - 添加语音输入按钮，复用现有按钮控件
  - 添加实时识别结果显示功能，将识别结果显示在聊天输入框
  - 实现新的交互模式：开始语音采集后实时识别，点击停止按钮结束
  - 保持与现有UI风格的一致性

### 2.3 音频录制与处理
- 使用Web Audio API录制音频，支持16kHz采样率、16位位深、单声道
- **核心实现**：
  - **Web Audio API录音**：使用`AudioContext`和`MediaRecorder`录制音频
  - **音频格式转换**：将录制的Float32Array数据转换为16位PCM数据
  - **WAV文件封装**：
    - 每次发送音频块时，都添加完整的WAV文件头
    - 确保每个音频块都是一个完整的WAV文件片段
    - 这样，即使按固定大小分割，每个块都是完整的格式
  - **分块处理**：
    - 每100-200ms发送一次音频数据，确保实时性
    - 每次发送的音频块都包含完整的WAV文件头
  - **权限处理**：处理录音权限请求和设备选择

### 2.4 语音停顿检测
- 实现语音停顿检测算法，优化音频分割
- **核心实现**：
  - **音频能量计算**：实时计算音频能量
  - **语音检测**：当音频能量超过阈值时，认为检测到语音
  - **停顿检测**：当音频能量低于阈值持续一定时间（如2秒）时，认为检测到停顿
  - **自然分割**：在语音停顿时，发送最后一个音频块并标记为结束
  - **优点**：
    - 确保语音的自然分割，提高识别准确性
    - 避免在单词或句子中间分割，减少识别错误
    - 优化网络传输，减少不必要的音频块发送

### 2.5 WebSocket通信
- 实现WebSocket客户端，与后端建立实时连接
- **核心实现**：
  - 建立WebSocket连接，处理连接状态
  - 分块发送音频数据，控制发送频率
  - 实时接收识别结果，更新输入框内容
  - 处理连接断开和重连逻辑

## 3. 技术栈

### 3.1 后端技术
- Python 3.8+
- FastAPI
- WebSocket (aiohttp)
- 豆包语音识别API
- 异步编程

### 3.2 前端技术
- React 18+
- TypeScript
- Web Audio API
- WebSocket
- Ant Design

## 4. 交互流程

1. **开始录音**：用户点击语音输入按钮，前端请求录音权限并开始录音
2. **建立连接**：前端与后端建立WebSocket连接
3. **流式传输**：前端将音频数据分块发送到后端
   - 每100-200ms发送一次音频数据
   - **关键技术**：每次发送的音频块都包含完整的WAV文件头
   - 数据格式为16kHz采样率、16位位深、单声道的WAV格式
4. **实时识别**：后端实时处理音频数据并返回识别结果
   - 后端将音频数据转发给豆包WebSocket API
   - 接收豆包返回的实时识别结果
   - 将识别结果实时转发给前端
5. **语音停顿检测**：前端实时检测语音停顿
   - 当检测到停顿时，发送最后一个音频块并标记为结束
   - 后端接收到结束标记后，处理完所有音频数据并返回最终识别结果
6. **显示结果**：前端将识别结果实时显示在聊天输入框
   - 实时更新输入框内容
   - 提供视觉反馈，显示识别进度
7. **停止录音**：用户点击停止按钮，前端停止录音并发送结束信号
8. **完成识别**：后端处理完所有音频数据并返回最终识别结果
9. **关闭连接**：前端与后端关闭WebSocket连接
10. **发送消息**：用户确认识别结果后，点击发送按钮发送消息

## 5. 配置与部署

### 5.1 配置修改
- 修改`config.py`中的默认语音服务提供商为`doubao_streaming`
- 确保`.env`文件中包含正确的豆包语音识别认证信息
- 保留whisper配置，确保其仍然可用

### 5.2 测试
- 编写后端测试脚本验证流式语音识别功能
- 测试前端录音和WebSocket通信功能
- 确保所有语音服务提供商都能正常工作
- 测试不同网络环境下的性能

## 6. 注意事项

- **前端录音**：使用Web Audio API实现前端录音，避免依赖pyaudio
  - **具体实现**：使用`navigator.mediaDevices.getUserMedia`获取麦克风权限，使用`AudioContext`处理音频数据
- **音频格式**：确保前端发送的音频格式与后端要求一致
  - **具体实现**：
    - 实现音频格式转换函数，将录制的音频数据转换为16kHz、16位、单声道的WAV格式
    - 每次发送音频块时，都添加完整的WAV文件头
    - 确保每个音频块都是一个完整的WAV文件片段
- **网络延迟**：处理网络延迟问题，确保实时识别的流畅性
  - **具体实现**：使用较小的音频块大小，优化WebSocket传输，实现缓冲机制
- **错误处理**：实现完善的错误处理机制，确保系统稳定性
  - **具体实现**：处理网络错误、音频设备错误、API错误等场景，提供友好的错误提示
- **兼容性**：确保在主流浏览器中都能正常工作
  - **具体实现**：使用标准Web API，避免使用实验性特性，提供降级方案
- **性能优化**：优化音频数据传输和处理，减少延迟
  - **具体实现**：实现音频数据压缩，使用高效的编码格式，优化WebSocket配置
- **用户体验**：提供清晰的视觉反馈，确保用户知道系统状态
  - **具体实现**：添加录音状态指示、语音波形可视化、识别进度显示等

## 7. 代码修改范围

- **后端**：
  - `backend/voice/doubaoVoice/`：添加流式语音识别集成
  - `backend/voice/voice_api.py`：添加WebSocket API端点
  - `backend/voice/config.py`：修改配置选项
  - `backend/voice/__init__.py`：更新服务提供商注册

- **前端**：
  - `frontend/src/components/voice/`：创建语音输入组件
  - `frontend/src/components/chat/ChatPanel.tsx`：修改输入区域，添加语音输入按钮
  - `frontend/src/utils/audio/`：添加音频处理工具函数

- **配置**：
  - `.env`：确保包含正确的豆包语音识别认证信息

## 8. 前端音频处理详细实现

### 8.1 Web Audio API录音实现
- **核心代码**：
  ```typescript
  // 获取麦克风权限
  const stream = await navigator.mediaDevices.getUserMedia({ audio: { sampleRate: 16000, channelCount: 1 } });
  
  // 创建AudioContext
  const audioContext = new AudioContext({ sampleRate: 16000 });
  const mediaStreamSource = audioContext.createMediaStreamSource(stream);
  
  // 创建ScriptProcessorNode处理音频数据
  const scriptNode = audioContext.createScriptProcessor(4096, 1, 1);
  
  // 处理音频数据
  scriptNode.onaudioprocess = (audioProcessingEvent) => {
    const inputData = audioProcessingEvent.inputBuffer.getChannelData(0);
    // 处理音频数据...
  };
  ```

### 8.2 音频格式转换实现
- **核心代码**：
  ```typescript
  // 将Float32Array转换为16位PCM
  function floatTo16BitPCM(input: Float32Array): Int16Array {
    const output = new Int16Array(input.length);
    for (let i = 0; i < input.length; i++) {
      const s = Math.max(-1, Math.min(1, input[i]));
      output[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return output;
  }
  ```

### 8.3 WAV文件封装实现
- **核心代码**：
  ```typescript
  // 生成WAV文件头
  function createWavHeader(dataSize: number, sampleRate: number = 16000, channels: number = 1, bitsPerSample: number = 16): Uint8Array {
    const header = new Uint8Array(44);
    // RIFF标识
    header.set(new TextEncoder().encode('RIFF'), 0);
    // 文件大小
    header.set(new DataView(new ArrayBuffer(4)).setUint32(0, 36 + dataSize, true).buffer, 4);
    // WAVE标识
    header.set(new TextEncoder().encode('WAVE'), 8);
    // fmt标识
    header.set(new TextEncoder().encode('fmt '), 12);
    // fmt块大小
    header.set(new DataView(new ArrayBuffer(4)).setUint32(0, 16, true).buffer, 16);
    // 格式类型
    header.set(new DataView(new ArrayBuffer(2)).setUint16(0, 1, true).buffer, 20);
    // 声道数
    header.set(new DataView(new ArrayBuffer(2)).setUint16(0, channels, true).buffer, 22);
    // 采样率
    header.set(new DataView(new ArrayBuffer(4)).setUint32(0, sampleRate, true).buffer, 24);
    // 字节率
    header.set(new DataView(new ArrayBuffer(4)).setUint32(0, sampleRate * channels * bitsPerSample / 8, true).buffer, 28);
    // 块对齐
    header.set(new DataView(new ArrayBuffer(2)).setUint16(0, channels * bitsPerSample / 8, true).buffer, 32);
    // 位深度
    header.set(new DataView(new ArrayBuffer(2)).setUint16(0, bitsPerSample, true).buffer, 34);
    // data标识
    header.set(new TextEncoder().encode('data'), 36);
    // 数据大小
    header.set(new DataView(new ArrayBuffer(4)).setUint32(0, dataSize, true).buffer, 40);
    return header;
  }
  
  // 封装完整的WAV音频块
  function createWavChunk(pcmData: Int16Array): Uint8Array {
    const dataSize = pcmData.length * 2; // 16位PCM，每个样本2字节
    const header = createWavHeader(dataSize);
    const wavData = new Uint8Array(header.length + dataSize);
    wavData.set(header, 0);
    
    // 将Int16Array转换为Uint8Array
    for (let i = 0; i < pcmData.length; i++) {
      wavData[header.length + i * 2] = pcmData[i] & 0xFF;
      wavData[header.length + i * 2 + 1] = (pcmData[i] >> 8) & 0xFF;
    }
    
    return wavData;
  }
  ```

### 8.4 语音停顿检测实现
- **核心代码**：
  ```typescript
  class VoiceActivityDetector {
    private silenceThreshold: number = 1000; // 静音阈值
    private silenceDuration: number = 2.0; // 静音持续时间（秒）
    private silenceCounter: number = 0; // 静音计数器
    private isSpeaking: boolean = false; // 是否正在说话
    
    // 计算音频能量
    calculateEnergy(data: Float32Array): number {
      let sum = 0;
      for (let i = 0; i < data.length; i++) {
        sum += Math.abs(data[i]);
      }
      return sum / data.length * 32768; // 转换为16位范围
    }
    
    // 检测语音和停顿
    detect(data: Float32Array, sampleRate: number = 16000): { isSpeaking: boolean, isSilence: boolean } {
      const energy = this.calculateEnergy(data);
      const frameDuration = data.length / sampleRate; // 每帧持续时间
      
      if (energy > this.silenceThreshold) {
        // 检测到语音
        this.isSpeaking = true;
        this.silenceCounter = 0;
        return { isSpeaking: true, isSilence: false };
      } else {
        // 检测到静音
        if (this.isSpeaking) {
          this.silenceCounter += frameDuration;
          if (this.silenceCounter >= this.silenceDuration) {
            // 检测到停顿
            this.isSpeaking = false;
            this.silenceCounter = 0;
            return { isSpeaking: false, isSilence: true };
          }
        }
        return { isSpeaking: this.isSpeaking, isSilence: false };
      }
    }
  }
  ```

### 8.5 WebSocket通信实现
- **核心代码**：
  ```typescript
  // 建立WebSocket连接
  const socket = new WebSocket('ws://localhost:8000/api/v1/voice/stream');
  
  // 连接建立
  socket.onopen = () => {
    console.log('WebSocket连接已建立');
  };
  
  // 接收消息
  socket.onmessage = (event) => {
    const data = JSON.parse(event.data);
    if (data.type === 'recognition_result') {
      setInputValue(data.text);
    }
  };
  
  // 发送音频数据
  function sendAudioData(audioData: Uint8Array) {
    socket.send(audioData);
  }
  ```

## 9. 后端WebSocket处理详细实现

### 9.1 WebSocket端点实现
- **核心代码**：
  ```python
  @router.websocket("/stream")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    try:
        # 初始化豆包流式语音识别客户端
        client = DoubaoStreamingVoiceIntegration()
        # 处理音频数据和识别结果
        await client.handle_stream(websocket)
    except Exception as e:
        logger.error(f"WebSocket错误: {e}")
        await websocket.close(code=1000, reason=str(e))
  ```

### 9.2 流式识别处理实现
- **核心代码**：
  ```python
  async def handle_stream(self, websocket: WebSocket):
    # 建立与豆包的WebSocket连接
    async with self._connect_to_doubao() as doubao_ws:
        # 发送初始化请求
        await self._send_initial_request(doubao_ws)
        
        # 启动双向通信
        async def receive_from_frontend():
            while True:
                data = await websocket.receive_bytes()
                # 转发音频数据给豆包
                await doubao_ws.send_bytes(data)
        
        async def send_to_frontend():
            while True:
                response = await doubao_ws.receive_bytes()
                # 解析豆包返回的结果
                result = self._parse_doubao_response(response)
                # 发送识别结果给前端
                await websocket.send_json({"type": "recognition_result", "text": result})
        
        # 并行处理
        await asyncio.gather(
            receive_from_frontend(),
            send_to_frontend()
        )
  ```